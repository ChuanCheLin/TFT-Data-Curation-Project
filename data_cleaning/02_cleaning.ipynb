{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose & Scope â€“ `02_cleaning.ipynb`\n",
    "\n",
    "This notebook carries out the **dataâ€“quality cleaning phase** of our TFT project.  \n",
    "Starting from the integrated unit-level table produced in `01_data_integration.ipynb`, we address:\n",
    "\n",
    "1. **Profiling & completeness** (Week 5â€“6)  \n",
    "2. **Duplicate detection** (Week 8)  \n",
    "3. **Rule-based and pattern-based repairs** (Week 10â€“11)  \n",
    "4. **Outlier / anomaly handling** (Week 7)  \n",
    "5. **Residual unmatched champions** (integration refinement, Week 13)  \n",
    "6. **Provenance & reproducibility** (Week 14â€“15)\n",
    "\n",
    "The cleaned dataset will be stored as `tft_units_long_clean.parquet` (or `.csv.gz` fallback) for downstream modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3\n",
      "pandas: 2.2.3\n",
      "Loaded shape: (3591, 11)\n"
     ]
    }
   ],
   "source": [
    "# Imports & environment snapshot\n",
    "import sys, platform, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"pandas:\", pd.__version__)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Project paths\n",
    "PROJ_ROOT   = Path.cwd().parent\n",
    "CURATED_DIR = PROJ_ROOT / \"curated\"\n",
    "\n",
    "# Safe load (parquet preferred)\n",
    "def safe_load(base: Path):\n",
    "    \"\"\"Load <base>.parquet if present, else <base>.csv.gz.\"\"\"\n",
    "    pq = base.with_suffix(\".parquet\")\n",
    "    gz = base.with_suffix(\".csv.gz\")\n",
    "    if pq.exists():\n",
    "        return pd.read_parquet(pq)\n",
    "    elif gz.exists():\n",
    "        return pd.read_csv(gz, compression=\"gzip\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No {pq.name} or {gz.name} found.\")\n",
    "\n",
    "# Load integrated table\n",
    "units = safe_load(CURATED_DIR / \"tft_units_long\")\n",
    "print(\"Loaded shape:\", units.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_ratio</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>mem_MB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0.220273</td>\n",
       "      <td>85</td>\n",
       "      <td>0.179812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.220273</td>\n",
       "      <td>85</td>\n",
       "      <td>0.179566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>0.220273</td>\n",
       "      <td>85</td>\n",
       "      <td>0.214346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>championId</th>\n",
       "      <td>0.220273</td>\n",
       "      <td>85</td>\n",
       "      <td>0.028728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>championKey</th>\n",
       "      <td>0.007519</td>\n",
       "      <td>108</td>\n",
       "      <td>0.197765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_id</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>50</td>\n",
       "      <td>0.226233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puuid</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>334</td>\n",
       "      <td>0.455082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>units</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>400</td>\n",
       "      <td>0.476420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star_level</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.028728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit_list</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>277</td>\n",
       "      <td>0.205646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>champRaw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.198464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             missing_ratio  n_unique    mem_MB\n",
       "name              0.220273        85  0.179812\n",
       "id                0.220273        85  0.179566\n",
       "title             0.220273        85  0.214346\n",
       "championId        0.220273        85  0.028728\n",
       "championKey       0.007519       108  0.197765\n",
       "match_id          0.000000        50  0.226233\n",
       "puuid             0.000000       334  0.455082\n",
       "units             0.000000       400  0.476420\n",
       "star_level        0.000000         4  0.028728\n",
       "unit_list         0.000000       277  0.205646\n",
       "champRaw          0.000000       109  0.198464"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ðŸ“Œ Quick profiling summary\n",
    "summary = (\n",
    "    units.isna().mean().to_frame(\"missing_ratio\")\n",
    "         .join(units.nunique().to_frame(\"n_unique\"))\n",
    "         .join((units.memory_usage(deep=True) / 1e6)\n",
    "               .to_frame(\"mem_MB\"))\n",
    ")\n",
    "display(summary.sort_values(\"missing_ratio\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicate rows: 28\n",
      "Duplicate (match_id, puuid, champRaw): 33\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Row-level duplication\n",
    "dup_rows = units.duplicated().sum()\n",
    "print(\"Exact duplicate rows:\", dup_rows)\n",
    "\n",
    "# ðŸ“Œ Logical key duplication: match_id + puuid + champRaw\n",
    "dup_keys = units.duplicated(subset=[\"match_id\", \"puuid\", \"champRaw\"]).sum()\n",
    "print(\"Duplicate (match_id, puuid, champRaw):\", dup_keys)\n",
    "\n",
    "# Drop if any exact duplicates\n",
    "if dup_rows:\n",
    "    units = units.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based & Pattern-based Repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid star_level rows: 9\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Star level must be 1â€“3\n",
    "invalid_star = units[~units[\"star_level\"].between(1, 3)]\n",
    "print(\"Invalid star_level rows:\", invalid_star.shape[0])\n",
    "\n",
    "# Repair policy: clip to nearest valid value\n",
    "units.loc[:, \"star_level\"] = units[\"star_level\"].clip(1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved champion ref as gzip-CSV: champ_ref.csv.gz\n",
      "Champion reference rebuilt from Data-Dragon.\n",
      "Champion rows: 170\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Champion reference loader with gzip-CSV fallback\n",
    "# ------------------------------------------------------------\n",
    "def load_champion_ref(raw_dir: Path, version: str = \"15.7.1\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a champion reference DataFrame from Data-Dragon JSON.\n",
    "    \"\"\"\n",
    "    champ_json = next((raw_dir / f\"dd_{version}\").rglob(\"data/en_US/champion.json\"))\n",
    "    with open(champ_json) as f:\n",
    "        raw = json.load(f)[\"data\"]\n",
    "    return (\n",
    "        pd.json_normalize(raw.values())\n",
    "          .assign(championId=lambda d: d[\"key\"].astype(int))\n",
    "          .loc[:, [\"championId\", \"name\", \"id\", \"title\"]]\n",
    "    )\n",
    "\n",
    "def safe_save(df: pd.DataFrame, base: Path, index=False) -> Path:\n",
    "    \"\"\"\n",
    "    Save as <base>.parquet if pyarrow/fastparquet available,\n",
    "    else as <base>.csv.gz. Return path actually written.\n",
    "    \"\"\"\n",
    "    pq = base.with_suffix(\".parquet\")\n",
    "    gz = base.with_suffix(\".csv.gz\")\n",
    "    try:\n",
    "        df.to_parquet(pq, index=index)\n",
    "        print(\"Saved champion ref as Parquet:\", pq.name)\n",
    "        return pq\n",
    "    except ImportError:\n",
    "        df.to_csv(gz, index=index, compression=\"gzip\")\n",
    "        print(\"Saved champion ref as gzip-CSV:\", gz.name)\n",
    "        return gz\n",
    "\n",
    "def safe_load(base: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Try loading <base>.parquet; if not present, try <base>.csv.gz.\n",
    "    Return None if neither exists.\n",
    "    \"\"\"\n",
    "    pq = base.with_suffix(\".parquet\")\n",
    "    gz = base.with_suffix(\".csv.gz\")\n",
    "    if pq.exists():\n",
    "        return pd.read_parquet(pq)\n",
    "    elif gz.exists():\n",
    "        return pd.read_csv(gz, compression=\"gzip\")\n",
    "    return None\n",
    "\n",
    "# -------------------- main logic ----------------------------\n",
    "champ_ref_base = CURATED_DIR / \"champ_ref\"\n",
    "champ_df = safe_load(champ_ref_base)\n",
    "\n",
    "if champ_df is not None:\n",
    "    print(\"Champion reference loaded from disk.\")\n",
    "else:\n",
    "    RAW_DIR = PROJ_ROOT / \"raw\"\n",
    "    champ_df = load_champion_ref(RAW_DIR, version=\"15.7.1\")\n",
    "    safe_save(champ_df, champ_ref_base, index=False)\n",
    "    print(\"Champion reference rebuilt from Data-Dragon.\")\n",
    "\n",
    "print(\"Champion rows:\", champ_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top unmatched champRaw:\n",
      " champRaw\n",
      "Rhaast    27\n",
      "Name: count, dtype: int64\n",
      "Remaining unmatched rows: 751\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 0) Build a mask for \"still unmatched\" rows (robust)\n",
    "# ------------------------------------------------------------\n",
    "if \"name\" in units.columns:\n",
    "    unmatched_mask = units[\"name\"].isna()\n",
    "elif \"championId\" in units.columns:\n",
    "    unmatched_mask = units[\"championId\"].isna()\n",
    "else:\n",
    "    # last-resort fallback: assume rows lacking a valid merge have NaN championKey\n",
    "    unmatched_mask = units[\"championKey\"].isna()\n",
    "\n",
    "# short-circuit if nothing is unmatched\n",
    "if unmatched_mask.sum() == 0:\n",
    "    print(\"No unmatched champions found.\")\n",
    "else:\n",
    "    # --------------------------------------------------------\n",
    "    # 1) Top 15 raw names that failed to match\n",
    "    # --------------------------------------------------------\n",
    "    unmatched_top = (\n",
    "        units.loc[unmatched_mask, \"champRaw\"]\n",
    "             .value_counts()\n",
    "             .head(15)\n",
    "    )\n",
    "    print(\"Top unmatched champRaw:\\n\", unmatched_top)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Regex-based quick fixes\n",
    "    # --------------------------------------------------------\n",
    "    fix_map = {\n",
    "        r\"^LeBlanc.*\":  \"LeBlanc\",\n",
    "        r\"^Swain.*\":    \"Swain\",\n",
    "        r\"^elise$\":     \"Elise\",\n",
    "        r\"^Rhaast$\":    \"Kayn\",\n",
    "        r\"^jinx$\":      \"Jinx\",\n",
    "        r\"^DrMundo$\":   \"DrMundo\",\n",
    "    }\n",
    "    for pat, repl in fix_map.items():\n",
    "        mask = unmatched_mask & units[\"champRaw\"].str.match(pat, case=False)\n",
    "        units.loc[mask, \"championKey\"] = repl\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3) Drop any old metadata columns that may exist\n",
    "    # --------------------------------------------------------\n",
    "    cols_to_drop = [c for c in [\"championId\", \"name\", \"id\", \"title\"]\n",
    "                    if c in units.columns]\n",
    "    units = units.drop(columns=cols_to_drop)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 4) Re-merge with champion reference\n",
    "    # --------------------------------------------------------\n",
    "    units = units.merge(\n",
    "        champ_df,\n",
    "        how=\"left\",\n",
    "        left_on=\"championKey\",\n",
    "        right_on=\"id\",\n",
    "        suffixes=(\"\", \"_champ\")\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5) Final unmatched count & placeholder\n",
    "    # --------------------------------------------------------\n",
    "    final_unmatched = units[\"name\"].isna().sum()\n",
    "    print(\"Remaining unmatched rows:\", final_unmatched)\n",
    "\n",
    "    units[\"championId\"] = units[\"championId\"].fillna(-1)\n",
    "    units[\"name\"]       = units[\"name\"].fillna(\"UNKNOWN_UNIT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched rows after auto-normalise: 461\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# Bulk normalisation for remaining unmatched champions\n",
    "# -------------------------------------------------------\n",
    "import re\n",
    "\n",
    "# 1) Build look-up sets for quick membership tests\n",
    "champ_names   = set(champ_df[\"id\"].str.lower())        # ids like 'Aatrox'\n",
    "alias_to_id   = {\"rhaast\": \"kayn\"}                     # manual alias dict (extend!)\n",
    "common_strip  = r\"(cougar|dragon|mega|star|summon|prime)$\"  # common suffixes\n",
    "\n",
    "def auto_normalise(raw: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Best-effort conversion of TFT unit string to base champion id.\n",
    "    Returns champion id if recognised, else None.\n",
    "    \"\"\"\n",
    "    s = raw.lower()\n",
    "    s = re.sub(r\"^tft\\d+_\", \"\", s)          # strip 'TFT14_'\n",
    "    s = re.sub(r\"\\d+$\", \"\", s)              # drop trailing digit\n",
    "    s = re.sub(common_strip, \"\", s)         # drop suffixes\n",
    "    s = s.strip()\n",
    "    \n",
    "    # manual alias\n",
    "    if s in alias_to_id:                        \n",
    "        return alias_to_id[s]\n",
    "    \n",
    "    # direct match\n",
    "    if s in champ_names:\n",
    "        return s.title()                    # restore leading capital\n",
    "    return None\n",
    "\n",
    "# 2) Apply to unmatched rows only\n",
    "mask_unmatched = units[\"name\"].eq(\"UNKNOWN_UNIT\")\n",
    "units.loc[mask_unmatched, \"championKey\"] = (\n",
    "    units.loc[mask_unmatched, \"champRaw\"]\n",
    "         .apply(auto_normalise)\n",
    "         .fillna(units.loc[mask_unmatched, \"championKey\"])   # keep existing value if still None\n",
    ")\n",
    "\n",
    "# 3) Drop old metadata cols (if present) and re-merge again\n",
    "cols_to_drop = [c for c in [\"championId\", \"name\", \"id\", \"title\"] if c in units.columns]\n",
    "units = units.drop(columns=cols_to_drop)\n",
    "\n",
    "units = units.merge(\n",
    "    champ_df, how=\"left\",\n",
    "    left_on=\"championKey\", right_on=\"id\",\n",
    "    suffixes=(\"\", \"_champ\")\n",
    ")\n",
    "\n",
    "# 4) Final unmatched count\n",
    "final_unmatched = units[\"name\"].isna().sum()\n",
    "print(\"Unmatched rows after auto-normalise:\", final_unmatched)\n",
    "\n",
    "# Mark residuals\n",
    "units[\"championId\"] = units[\"championId\"].fillna(-1)\n",
    "units[\"name\"]       = units[\"name\"].fillna(\"UNKNOWN_UNIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the 461 residual â€œUNKNOWN_UNITâ€ rows  \n",
    "\n",
    "After automatic normalisation, **461 / 3 591 = 12.8 %** of unit records still have no official champion mapping.  \n",
    "A spot check shows they are:\n",
    "\n",
    "* PvE / â€œminibossâ€ tokens (e.g., *Beardy*, *Blue*, *Lieutenant*)  \n",
    "* Temporary summons (*JayceSummon*, *LuxLaser*)  \n",
    "* Silco (non-LoL character) and other TFT-only specials\n",
    "\n",
    "Because these entities **do not correspond to real champions**, keeping them would only\n",
    "1. distort champion-level analyses, and  \n",
    "2. complicate feature engineering (no stats in Data Dragon).\n",
    "\n",
    "**Action** â€“ we drop them from the cleaned dataset, record the row count in the provenance\n",
    "meta file, and mention the rationale in the final report (Week 6: Missing values, Week 13: Knowledge-based cleaning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 461 residual non-champion rows (12.9% of the table).\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Drop residual UNKNOWN_UNIT rows and log the removal\n",
    "before_rows = units.shape[0]\n",
    "\n",
    "units = units[units[\"name\"] != \"UNKNOWN_UNIT\"].reset_index(drop=True)\n",
    "\n",
    "after_rows  = units.shape[0]\n",
    "dropped     = before_rows - after_rows\n",
    "print(f\"Dropped {dropped} residual non-champion rows \"\n",
    "      f\"({dropped / before_rows:.1%} of the table).\")\n",
    "\n",
    "# >>> continue with outlier flagging / safe_save / meta-json â€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tft_units_long_clean.csv.gz\n",
      "Meta written.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Re-use safe_save from previous notebook\n",
    "def safe_save(df, out_base: Path, index=False):\n",
    "    pq = out_base.with_suffix(\".parquet\")\n",
    "    gz = out_base.with_suffix(\".csv.gz\")\n",
    "    try:\n",
    "        df.to_parquet(pq, index=index)\n",
    "        print(\"Saved:\", pq.name)\n",
    "        return pq\n",
    "    except ImportError:\n",
    "        df.to_csv(gz, index=index, compression=\"gzip\")\n",
    "        print(\"Saved:\", gz.name)\n",
    "        return gz\n",
    "\n",
    "out_base = CURATED_DIR / \"tft_units_long_clean\"\n",
    "out_path = safe_save(units, out_base, index=False)\n",
    "\n",
    "# ðŸ“Œ Minimal provenance JSON\n",
    "meta = {\n",
    "    \"generated_at\": pd.Timestamp.utcnow().isoformat(),\n",
    "    \"input_file\":   \"tft_units_long.*\",\n",
    "    \"output_file\":  out_path.name,\n",
    "    \"rows\":         int(units.shape[0]),\n",
    "    \"unmatched\":    int((units['name'] == 'UNKNOWN_UNIT').sum()),\n",
    "}\n",
    "with open(out_base.with_suffix(\".meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Meta written.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
